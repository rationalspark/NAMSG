ARSG is an efficient method for training neural networks. The acronym is derived from an adaptive remote stochastic gradient method. In training logistic regression on MNIST and Resnet-20 on CIFAR10 with fixed optimal hyper-parameters obtained by grid search, ARSG roughly halves the computation compared with ADAM. In the experiments, it also outperforms RANGER, which is a promising adaptive method proposed very recently by combining RADAM and the lookahead optimizer. 

The paper is available at https://arxiv.org/abs/1905.01422. 

NAMSG is a former name of ARSG.
